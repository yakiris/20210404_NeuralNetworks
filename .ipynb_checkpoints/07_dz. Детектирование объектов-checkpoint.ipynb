{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/brendan45774/chess-piece-dectection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_annots_path = '../data/Chess_Piece_Dectection/annotations/'\n",
    "root_images_path = '../data/Chess_Piece_Dectection/images/'\n",
    "\n",
    "annots_path = sorted([i for i in Path(root_annots_path).glob('*.xml')])\n",
    "images_path = sorted([i for i in Path(root_images_path).glob('*.png')])\n",
    "\n",
    "n_imgs = len(images_path)\n",
    "\n",
    "classes = np.array([\"Rook\", \"Bishop\", \"Pawn\", \"Knight\", \"Queen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<annotation>\n",
      "    <folder>images</folder>\n",
      "    <filename>chess13.png</filename>\n",
      "    <size>\n",
      "        <width>400</width>\n",
      "        <height>225</height>\n",
      "        <depth>3</depth>\n",
      "    </size>\n",
      "    <segmented>0</segmented>\n",
      "    <object>\n",
      "        <name>white-queen</name>\n",
      "        <pose>Unspecified</pose>\n",
      "        <truncated>0</truncated>\n",
      "        <occluded>0</occluded>\n",
      "        <difficult>0</difficult>\n",
      "        <bndbox>\n",
      "            <xmin>139</xmin>\n",
      "            <ymin>16</ymin>\n",
      "            <xmax>201</xmax>\n",
      "            <ymax>154</ymax>\n",
      "        </bndbox>\n",
      "    </object>\n",
      "</annotation>\n"
     ]
    }
   ],
   "source": [
    "with open(annots_path[5], 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('imageai/data/train/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/train/annotations', exist_ok=True)\n",
    "\n",
    "os.makedirs('imageai/data/validation/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/validation/annotations', exist_ok=True)\n",
    "\n",
    "os.makedirs('imageai/data/test/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/test/annotations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs = 81\n",
    "n_split = n_imgs // 6\n",
    "\n",
    "\n",
    "for i, (annot_path, img_path) in enumerate(zip(annots_path, images_path)):    \n",
    "    if i > n_imgs:\n",
    "        break\n",
    "    # train-val-test split\n",
    "    if i < n_split:\n",
    "        shutil.copy(img_path, 'imageai/data/test/images/' + img_path.parts[-1])\n",
    "        shutil.copy(annot_path, 'imageai/data/test/annotations/' + annot_path.parts[-1])\n",
    "    elif n_split <= i < n_split*2:\n",
    "        shutil.copy(img_path, 'imageai/data/validation/images/' + img_path.parts[-1])\n",
    "        shutil.copy(annot_path, 'imageai/data/validation/annotations/' + annot_path.parts[-1])\n",
    "    else:\n",
    "        shutil.copy(img_path, 'imageai/data/train/images/' + img_path.parts[-1])\n",
    "        shutil.copy(annot_path, 'imageai/data/train/annotations/' + annot_path.parts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(list(Path('imageai/data/train/annotations/').glob('*.xml'))))\n",
    "print(len(list(Path('imageai/data/validation/annotations/').glob('*.xml'))))\n",
    "print(len(list(Path('imageai/data/test/annotations/').glob('*.xml'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"../models/pretrained-yolov3.h5\")\n",
    "\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 0.80\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  ./imageai/data/json/detection_config.json\n",
      "Evaluating over 0 samples taken from ./imageai/data/validation\n",
      "Training over 55 samples  given at ./imageai/data/train\n",
      "Some labels have no annotations! Please revise the list of labels in your configuration.\n",
      "Training on: \tNone\n",
      "Training with Batch Size:  8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-95c3f8c0dee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#                        train_from_pretrained_model=\"imageai/data/models/detection_model-ex-009--loss-0024.110.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/imageai/Detection/Custom/__init__.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training on: \\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training with Batch Size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Training Samples: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Validation Samples: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Experiments: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=\"./imageai/data/\")\n",
    "trainer.setTrainConfig(object_names_array=classes,\n",
    "                       batch_size=8,\n",
    "                       num_experiments=10,\n",
    "                       train_from_pretrained_model=\"models/pretrained-yolov3.h5\")\n",
    "#                        train_from_pretrained_model=\"imageai/data/models/detection_model-ex-009--loss-0024.110.h5\")\n",
    "\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
